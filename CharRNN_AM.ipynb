{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from moses import CharVocab, StringDataset\n",
    "from moses.char_rnn import CharRNN\n",
    "from moses.char_rnn import config as CharRNNConfig\n",
    "import moses\n",
    "import torch\n",
    "from torch import autograd\n",
    "import rdkit.Chem as chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from moses.interfaces import MosesTrainer\n",
    "from moses.utils import CharVocab, Logger\n",
    "\n",
    "\n",
    "def anti_model_rnn_loss(loss, alpha=0.5):\n",
    "    loss = torch.exp(-1 * loss)\n",
    "    loss = 1. - loss + 1.e-7\n",
    "    loss = torch.log2(loss)\n",
    "    loss = -alpha * loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "class CharRNNTrainer(MosesTrainer):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def _train_epoch(self, model, tqdm_data, criterion, optimizer=None):\n",
    "        if optimizer is None:\n",
    "            model.eval()\n",
    "        else:\n",
    "            model.train()\n",
    "\n",
    "        postfix = {'loss': 0,\n",
    "                   'running_loss': 0}\n",
    "\n",
    "        for i, ((prevs, nexts, lens), (nprevs, nnexts, nlens)) in enumerate(tqdm_data):\n",
    "            prevs = prevs.to(model.device)\n",
    "            nexts = nexts.to(model.device)\n",
    "            lens = lens.to(model.device)\n",
    "            \n",
    "            nprevs = nprevs.to(model.device)\n",
    "            nnexts = nnexts.to(model.device)\n",
    "            nlens = nlens.to(model.device)\n",
    "            \n",
    "            outputs, _, _ = model(prevs, lens)\n",
    "            noutputs, _, _ = model(nprevs, nlens)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]),\n",
    "                             nexts.view(-1))\n",
    "            \n",
    "            nloss = criterion(noutputs.view(-1, noutputs.shape[-1]),\n",
    "                             nnexts.view(-1))\n",
    "            \n",
    "            nloss = anti_model_rnn_loss(loss)\n",
    "            loss = torch.mean(loss)\n",
    "            nloss = torch.mean(nloss)\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                nloss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            postfix['loss'] = loss.item() + nloss.item()\n",
    "            postfix['running_loss'] += (loss.item() + nloss.item() -\n",
    "                                        postfix['running_loss']) / (i + 1)\n",
    "            tqdm_data.set_postfix(postfix)\n",
    "\n",
    "        postfix['mode'] = 'Eval' if optimizer is None else 'Train'\n",
    "        return postfix\n",
    "\n",
    "    def _train(self, model, train_loader, negative_loader, val_loader=None, logger=None):\n",
    "        def get_params():\n",
    "            return (p for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        device = model.device\n",
    "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        optimizer = optim.Adam(get_params(), lr=self.config.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              self.config.step_size,\n",
    "                                              self.config.gamma)\n",
    "\n",
    "        model.zero_grad()\n",
    "        for epoch in range(self.config.train_epochs):\n",
    "\n",
    "            tqdm_data = tqdm(zip(train_loader, negative_loader),\n",
    "                             desc='Training (epoch #{})'.format(epoch))\n",
    "            postfix = self._train_epoch(model, tqdm_data, criterion, optimizer)\n",
    "            if logger is not None:\n",
    "                logger.append(postfix)\n",
    "                logger.save(self.config.log_file)\n",
    "\n",
    "            if val_loader is not None:\n",
    "                tqdm_data = tqdm(val_loader,\n",
    "                                 desc='Validation (epoch #{})'.format(epoch))\n",
    "                postfix = self._train_epoch(model, tqdm_data, criterion)\n",
    "                if logger is not None:\n",
    "                    logger.append(postfix)\n",
    "                    logger.save(self.config.log_file)\n",
    "\n",
    "            if (self.config.model_save is not None) and \\\n",
    "                    (epoch % self.config.save_frequency == 0):\n",
    "                model = model.to('cpu')\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    self.config.model_save[:-3]+'_{0:03d}.pt'.format(epoch)\n",
    "                )\n",
    "                model = model.to(device)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "    def get_vocabulary(self, data):\n",
    "        return CharVocab.from_data(data)\n",
    "\n",
    "    def get_collate_fn(self, model):\n",
    "        device = self.get_collate_device(model)\n",
    "\n",
    "        def collate(data):\n",
    "            data.sort(key=len, reverse=True)\n",
    "            tensors = [model.string2tensor(string, device=device)\n",
    "                       for string in data]\n",
    "\n",
    "            pad = model.vocabulary.pad\n",
    "            prevs = pad_sequence([t[:-1] for t in tensors],\n",
    "                                 batch_first=True, padding_value=pad)\n",
    "            nexts = pad_sequence([t[1:] for t in tensors],\n",
    "                                 batch_first=True, padding_value=pad)\n",
    "            lens = torch.tensor([len(t) - 1 for t in tensors],\n",
    "                                dtype=torch.long, device=device)\n",
    "            return prevs, nexts, lens\n",
    "\n",
    "        return collate\n",
    "\n",
    "    def fit(self, model, train_data, negative_data, val_data=None):\n",
    "        logger = Logger() if self.config.log_file is not None else None\n",
    "\n",
    "        train_loader = self.get_dataloader(model, train_data, shuffle=True)\n",
    "        negative_loader = self.get_dataloader(model, negative_data, shuffle=True)\n",
    "        \n",
    "        val_loader = None if val_data is None else self.get_dataloader(\n",
    "            model, val_data, shuffle=False\n",
    "        )\n",
    "\n",
    "        self._train(model, train_loader, negative_loader, val_loader, logger)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darius/anaconda3/envs/moses/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d86f82d1dfa43baabe6904ddd17b66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training (epoch #0)'), FloatProgress(value=0.0, max=1584663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8037, 0.2505, 1.0000, 0.2562, 0.0651, 0.9999, 1.0000, 1.0000, 0.9992,\n",
      "        0.1919, 1.0000, 0.6166, 0.9505, 0.1513, 1.0000, 0.8437, 0.7702, 0.8421,\n",
      "        0.4521, 0.3031, 0.4060, 0.1638, 1.0000, 0.9986, 1.0000, 1.0000, 0.9993,\n",
      "        1.0000, 0.8675, 0.0761, 0.9999, 1.0000], grad_fn=<ExpBackward>)\n",
      "tensor([1.9630e-01, 7.4952e-01, 3.3842e-07, 7.4379e-01, 9.3492e-01, 6.2327e-05,\n",
      "        1.0000e-07, 1.0000e-07, 7.7758e-04, 8.0806e-01, 3.7293e-05, 3.8339e-01,\n",
      "        4.9469e-02, 8.4867e-01, 1.6497e-06, 1.5630e-01, 2.2979e-01, 1.5789e-01,\n",
      "        5.4791e-01, 6.9692e-01, 5.9403e-01, 8.3618e-01, 1.6497e-06, 1.3759e-03,\n",
      "        2.8418e-06, 1.0000e-07, 7.0689e-04, 7.7294e-06, 1.3250e-01, 9.2391e-01,\n",
      "        5.9824e-05, 1.4763e-05], grad_fn=<AddBackward0>)\n",
      "tensor([ -2.3488,  -0.4160, -21.4947,  -0.4270,  -0.0971, -13.9698, -23.2535,\n",
      "        -23.2535, -10.3287,  -0.3075, -14.7107,  -1.3831,  -4.3373,  -0.2367,\n",
      "        -19.2093,  -2.6776,  -2.1216,  -2.6630,  -0.8680,  -0.5209,  -0.7514,\n",
      "         -0.2581, -19.2093,  -9.5054, -18.4248, -23.2535, -10.4662, -16.9812,\n",
      "         -2.9160,  -0.1142, -14.0289, -16.0477], grad_fn=<Log2Backward>)\n",
      "tensor([ 1.1744,  0.2080, 10.7473,  0.2135,  0.0485,  6.9849, 11.6267, 11.6267,\n",
      "         5.1644,  0.1537,  7.3554,  0.6916,  2.1687,  0.1184,  9.6047,  1.3388,\n",
      "         1.0608,  1.3315,  0.4340,  0.2605,  0.3757,  0.1291,  9.6047,  4.7527,\n",
      "         9.2124, 11.6267,  5.2331,  8.4906,  1.4580,  0.0571,  7.0145,  8.0238],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "tensor([0.7813, 0.2300, 1.0000, 0.2894, 0.4298, 0.6318, 0.0026, 0.8901, 0.9943,\n",
      "        0.4013, 1.0000, 1.0000, 1.0000, 0.9998, 0.9917, 0.9696, 0.9993, 1.0000,\n",
      "        1.0000, 0.9903, 0.9686, 0.8975, 0.9989, 1.0000, 1.0000, 1.0000, 0.9969,\n",
      "        0.4936, 0.3648, 0.2717, 1.0000, 0.6823, 0.7134, 0.8138, 0.2361, 0.3611,\n",
      "        0.8892, 1.0000, 1.0000, 0.9965, 1.0000, 0.9942], grad_fn=<ExpBackward>)\n",
      "tensor([2.1871e-01, 7.6997e-01, 1.1729e-06, 7.1056e-01, 5.7020e-01, 3.6823e-01,\n",
      "        9.9741e-01, 1.0986e-01, 5.6661e-03, 5.9870e-01, 3.3187e-06, 1.0000e-07,\n",
      "        1.0000e-07, 1.6997e-04, 8.2669e-03, 3.0365e-02, 6.9449e-04, 7.3718e-06,\n",
      "        6.2989e-06, 9.6664e-03, 3.1396e-02, 1.0254e-01, 1.0640e-03, 4.7784e-05,\n",
      "        3.0802e-06, 3.3842e-07, 3.0982e-03, 5.0643e-01, 6.3521e-01, 7.2826e-01,\n",
      "        1.8577e-05, 3.1769e-01, 2.8664e-01, 1.8619e-01, 7.6388e-01, 6.3892e-01,\n",
      "        1.1080e-01, 9.3447e-07, 4.1531e-06, 3.4939e-03, 3.7955e-06, 5.7964e-03],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([-2.1929e+00, -3.7713e-01, -1.9702e+01, -4.9296e-01, -8.1046e-01,\n",
      "        -1.4413e+00, -3.7392e-03, -3.1863e+00, -7.4634e+00, -7.4009e-01,\n",
      "        -1.8201e+01, -2.3253e+01, -2.3253e+01, -1.2522e+01, -6.9184e+00,\n",
      "        -5.0414e+00, -1.0492e+01, -1.7050e+01, -1.7276e+01, -6.6928e+00,\n",
      "        -4.9933e+00, -3.2857e+00, -9.8763e+00, -1.4353e+01, -1.8309e+01,\n",
      "        -2.1495e+01, -8.3343e+00, -9.8158e-01, -6.5469e-01, -4.5747e-01,\n",
      "        -1.5716e+01, -1.6543e+00, -1.8027e+00, -2.4252e+00, -3.8859e-01,\n",
      "        -6.4630e-01, -3.1740e+00, -2.0029e+01, -1.7877e+01, -8.1610e+00,\n",
      "        -1.8007e+01, -7.4306e+00], grad_fn=<Log2Backward>)\n",
      "tensor([1.0964e+00, 1.8856e-01, 9.8508e+00, 2.4648e-01, 4.0523e-01, 7.2066e-01,\n",
      "        1.8696e-03, 1.5931e+00, 3.7317e+00, 3.7004e-01, 9.1005e+00, 1.1627e+01,\n",
      "        1.1627e+01, 6.2612e+00, 3.4592e+00, 2.5207e+00, 5.2459e+00, 8.5248e+00,\n",
      "        8.6382e+00, 3.3464e+00, 2.4966e+00, 1.6429e+00, 4.9382e+00, 7.1766e+00,\n",
      "        9.1543e+00, 1.0747e+01, 4.1672e+00, 4.9079e-01, 3.2735e-01, 2.2874e-01,\n",
      "        7.8580e+00, 8.2716e-01, 9.0135e-01, 1.2126e+00, 1.9430e-01, 3.2315e-01,\n",
      "        1.5870e+00, 1.0015e+01, 8.9387e+00, 4.0805e+00, 9.0036e+00, 3.7153e+00],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "tensor([0.7547, 0.2470, 0.4297, 0.5562, 0.0456, 0.2916, 0.7574, 0.0159, 1.0000,\n",
      "        0.8764, 0.1110, 0.8778, 0.9438, 0.9032, 0.6959, 0.9989, 0.6543, 0.8672,\n",
      "        1.0000, 0.9999, 0.0127, 0.6885, 1.0000, 1.0000, 0.9436, 0.9778, 0.9870,\n",
      "        1.0000, 0.9997, 0.9999, 0.9975, 0.5732, 0.6838, 1.0000, 0.8832],\n",
      "       grad_fn=<ExpBackward>)\n",
      "tensor([2.4531e-01, 7.5301e-01, 5.7027e-01, 4.4382e-01, 9.5441e-01, 7.0844e-01,\n",
      "        2.4257e-01, 9.8411e-01, 1.2498e-05, 1.2361e-01, 8.8904e-01, 1.2222e-01,\n",
      "        5.6190e-02, 9.6769e-02, 3.0408e-01, 1.0841e-03, 3.4566e-01, 1.3284e-01,\n",
      "        3.6101e-05, 7.8063e-05, 9.8732e-01, 3.1150e-01, 1.4113e-06, 4.3915e-06,\n",
      "        5.6397e-02, 2.2185e-02, 1.3003e-02, 2.1921e-07, 2.6767e-04, 1.3338e-04,\n",
      "        2.5319e-03, 4.2684e-01, 3.1621e-01, 5.7684e-07, 1.1676e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([-2.0273e+00, -4.0926e-01, -8.1028e-01, -1.1719e+00, -6.7315e-02,\n",
      "        -4.9727e-01, -2.0435e+00, -2.3102e-02, -1.6288e+01, -3.0161e+00,\n",
      "        -1.6968e-01, -3.0324e+00, -4.1535e+00, -3.3693e+00, -1.7175e+00,\n",
      "        -9.8493e+00, -1.5326e+00, -2.9123e+00, -1.4758e+01, -1.3645e+01,\n",
      "        -1.8411e-02, -1.6827e+00, -1.9435e+01, -1.7797e+01, -4.1482e+00,\n",
      "        -5.4943e+00, -6.2650e+00, -2.2121e+01, -1.1867e+01, -1.2872e+01,\n",
      "        -8.6256e+00, -1.2282e+00, -1.6610e+00, -2.0725e+01, -3.0983e+00],\n",
      "       grad_fn=<Log2Backward>)\n",
      "tensor([1.0136e+00, 2.0463e-01, 4.0514e-01, 5.8597e-01, 3.3658e-02, 2.4864e-01,\n",
      "        1.0218e+00, 1.1551e-02, 8.1440e+00, 1.5081e+00, 8.4838e-02, 1.5162e+00,\n",
      "        2.0768e+00, 1.6847e+00, 8.5873e-01, 4.9247e+00, 7.6628e-01, 1.4561e+00,\n",
      "        7.3788e+00, 6.8225e+00, 9.2057e-03, 8.4135e-01, 9.7173e+00, 8.8984e+00,\n",
      "        2.0741e+00, 2.7471e+00, 3.1325e+00, 1.1061e+01, 5.9336e+00, 6.4361e+00,\n",
      "        4.3128e+00, 6.1412e-01, 8.3052e-01, 1.0363e+01, 1.5492e+00],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-311429a7050d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'crnn._060.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-9ecda4a07691>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_data, val_data)\u001b[0m\n\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9ecda4a07691>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, model, train_loader, val_loader, logger)\u001b[0m\n\u001b[1;32m     80\u001b[0m             tqdm_data = tqdm(train_loader,\n\u001b[1;32m     81\u001b[0m                              desc='Training (epoch #{})'.format(epoch))\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mpostfix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9ecda4a07691>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, model, tqdm_data, criterion, optimizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/darius/anaconda3/envs/moses/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/darius/anaconda3/envs/moses/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = moses.get_dataset('train')\n",
    "\n",
    "config = CharRNNConfig.get_config()\n",
    "config.log_file = None\n",
    "config.n_workers = 1\n",
    "config.n_batch = 1\n",
    "trainer = CharRNNTrainer(config)\n",
    "crnn = CharRNN(CharVocab.from_data(train), config)\n",
    "crnn.load_state_dict(torch.load('crnn._060.pt'))\n",
    "with autograd.detect_anomaly():\n",
    "    trainer.fit(crnn, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moses",
   "language": "python",
   "name": "moses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
