{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem as chem\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import moses\n",
    "from moses import CharVocab, StringDataset\n",
    "from moses.char_rnn import CharRNN\n",
    "from moses.char_rnn import config as CharRNNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from moses.interfaces import MosesTrainer\n",
    "from moses.utils import CharVocab, Logger\n",
    "\n",
    "\n",
    "def anti_model_rnn_loss(loss, alpha=1.0):\n",
    "    mask = loss == 0.0\n",
    "    loss = torch.exp(-1 * loss)\n",
    "    loss = 1.0 - loss + 1.0e-7\n",
    "    loss = torch.log(loss)\n",
    "    loss = -alpha * loss\n",
    "    loss[mask] = 0.0\n",
    "    return loss\n",
    "\n",
    "\n",
    "class CharRNNTrainer(MosesTrainer):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def _train_epoch(self, epoch, model, tqdm_data, criterion, optimizer=None):\n",
    "        if optimizer is None:\n",
    "            model.eval()\n",
    "        else:\n",
    "            model.train()\n",
    "\n",
    "        postfix = {\"loss\": 0, \"running_loss\": 0}\n",
    "\n",
    "        for i, ((prevs, nexts, lens), (nprevs, nnexts, nlens)) in enumerate(tqdm_data):\n",
    "            prevs = prevs.to(model.device)\n",
    "            nexts = nexts.to(model.device)\n",
    "            lens = lens.to(model.device)\n",
    "\n",
    "            nprevs = nprevs.to(model.device)\n",
    "            nnexts = nnexts.to(model.device)\n",
    "            nlens = nlens.to(model.device)\n",
    "\n",
    "            outputs, _, _ = model(prevs, lens)\n",
    "            noutputs, _, _ = model(nprevs, nlens)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), nexts.view(-1))\n",
    "\n",
    "            nloss = criterion(noutputs.view(-1, noutputs.shape[-1]), nnexts.view(-1))\n",
    "\n",
    "            nloss = anti_model_rnn_loss(nloss)\n",
    "            loss = torch.mean(loss)\n",
    "            nloss = torch.mean(nloss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                writer.add_scalar(\n",
    "                    \"Loss/ploss\",\n",
    "                    loss.exp(),\n",
    "                    i + epoch * len(tqdm_data),\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"Loss/nloss\",\n",
    "                    nloss.exp(),\n",
    "                    i + epoch * len(tqdm_data),\n",
    "                )\n",
    "\n",
    "            loss = loss + nloss\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            postfix[\"loss\"] = loss.item() + nloss.item()\n",
    "            postfix[\"running_loss\"] += (\n",
    "                loss.item() + nloss.item() - postfix[\"running_loss\"]\n",
    "            ) / (i + 1)\n",
    "            tqdm_data.set_postfix(postfix)\n",
    "\n",
    "        postfix[\"mode\"] = \"Eval\" if optimizer is None else \"Train\"\n",
    "        return postfix\n",
    "\n",
    "    def _train(\n",
    "        self, model, train_loader, negative_loader, val_loader=None, logger=None\n",
    "    ):\n",
    "        def get_params():\n",
    "            return (p for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        device = model.device\n",
    "        criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        optimizer = optim.Adam(get_params(), lr=self.config.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer, self.config.step_size, self.config.gamma\n",
    "        )\n",
    "\n",
    "        model.zero_grad()\n",
    "        for epoch in range(self.config.train_epochs):\n",
    "\n",
    "            tqdm_data = tqdm(\n",
    "                zip(train_loader, negative_loader),\n",
    "                desc=\"Training (epoch #{})\".format(epoch),\n",
    "                total=min(len(train_loader), len(negative_loader)),\n",
    "            )\n",
    "            postfix = self._train_epoch(epoch, model, tqdm_data, criterion, optimizer)\n",
    "            if logger is not None:\n",
    "                logger.append(postfix)\n",
    "                logger.save(self.config.log_file)\n",
    "\n",
    "            if val_loader is not None:\n",
    "                tqdm_data = tqdm(\n",
    "                    val_loader, desc=\"Validation (epoch #{})\".format(epoch)\n",
    "                )\n",
    "                postfix = self._train_epoch(model, tqdm_data, criterion)\n",
    "                if logger is not None:\n",
    "                    logger.append(postfix)\n",
    "                    logger.save(self.config.log_file)\n",
    "\n",
    "            if (self.config.model_save is not None) and (\n",
    "                epoch % self.config.save_frequency == 0\n",
    "            ):\n",
    "                model = model.to(\"cpu\")\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    self.config.model_save[:-3] + \"_{0:03d}.pt\".format(epoch),\n",
    "                )\n",
    "                model = model.to(device)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "    def get_vocabulary(self, data):\n",
    "        return CharVocab.from_data(data)\n",
    "\n",
    "    def get_collate_fn(self, model):\n",
    "        device = self.get_collate_device(model)\n",
    "\n",
    "        def collate(data):\n",
    "            data.sort(key=len, reverse=True)\n",
    "            tensors = [model.string2tensor(string, device=device) for string in data]\n",
    "\n",
    "            pad = model.vocabulary.pad\n",
    "            prevs = pad_sequence(\n",
    "                [t[:-1] for t in tensors], batch_first=True, padding_value=pad\n",
    "            )\n",
    "            nexts = pad_sequence(\n",
    "                [t[1:] for t in tensors], batch_first=True, padding_value=pad\n",
    "            )\n",
    "            lens = torch.tensor(\n",
    "                [len(t) - 1 for t in tensors], dtype=torch.long, device=device\n",
    "            )\n",
    "            return prevs, nexts, lens\n",
    "\n",
    "        return collate\n",
    "\n",
    "    def fit(self, model, train_data, negative_data, val_data=None):\n",
    "        logger = Logger() if self.config.log_file is not None else None\n",
    "\n",
    "        train_loader = self.get_dataloader(model, train_data, shuffle=True)\n",
    "        negative_loader = self.get_dataloader(model, negative_data, shuffle=True)\n",
    "\n",
    "        val_loader = (\n",
    "            None\n",
    "            if val_data is None\n",
    "            else self.get_dataloader(model, val_data, shuffle=False)\n",
    "        )\n",
    "\n",
    "        self._train(model, train_loader, negative_loader, val_loader, logger)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ngram_smiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_trigrams = df.loc[(df[\"n\"] == 3) | (df[\"n\"] == 4)][\"smiles\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e934a0e7213944b2b1924fceb4e6c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training (epoch #0)'), FloatProgress(value=0.0, max=12381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb65de1b594e48698704648aef5bdd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training (epoch #1)'), FloatProgress(value=0.0, max=12381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fb6078242b49499a728412770aba16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training (epoch #2)'), FloatProgress(value=0.0, max=12381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f46bb7aa9414e35b58b3a73c5c51634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training (epoch #3)'), FloatProgress(value=0.0, max=12381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5216e54890b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCharVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-1408cf63c1cc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_data, negative_data, val_data)\u001b[0m\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1408cf63c1cc>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, model, train_loader, negative_loader, val_loader, logger)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             )\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mpostfix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1408cf63c1cc>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch, model, tqdm_data, criterion, optimizer)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/moses/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/moses/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = moses.get_dataset(\"train\")\n",
    "neg = neg_trigrams\n",
    "\n",
    "config = CharRNNConfig.get_config()\n",
    "config.log_file = None\n",
    "config.n_workers = 1\n",
    "config.n_batch = 128\n",
    "config.model_save = \"crnn_am_1.pth\"\n",
    "config.save_frequency = 20\n",
    "trainer = CharRNNTrainer(config)\n",
    "crnn = CharRNN(CharVocab.from_data(train), config)\n",
    "crnn.cuda()\n",
    "trainer.fit(crnn, train, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals = crnn.sample(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moses.get_all_metrics(test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moses",
   "language": "python",
   "name": "moses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
